{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pywren\n",
    "import numpy as np\n",
    "from boto.s3.connection import S3Connection\n",
    "from boto.s3.key import Key\n",
    "import pywren.storage as storage\n",
    "import boto3\n",
    "import pickle \n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configuration \n",
    "num_workers = 3 \n",
    "bucket_name = 'terasort-yawen'\n",
    "\n",
    "# the file to be sorted should be partitioned into \"num_worker\" number of files \n",
    "# as inputs to the map stage; \n",
    "# specify directory that contains files to be sorted: input1, input2, etc. \n",
    "file_path_local = 'input_files/' \n",
    "file_name = 'input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "#boto3 already configured \n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# upload n input files to S3 (inputs to the mapper stage)\n",
    "for i in range(num_workers):\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open(file_path_local + file_name + str(i), 'rb'),\n",
    "        Key = file_name+str(i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records: 30\n",
      "10\n",
      "20\n",
      "29\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['GLSnlm0*P*', \"o7~drsiz'L\", '~sHd0jDv6X']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## sample[i âˆ’ 1] <= key < sample[i] is sent to reduce i\n",
    "def get_sample_keys(file_path, num_workers):\n",
    "    ## Open the file with read only permit\n",
    "    f = open(file_path, \"r\")\n",
    "\n",
    "    ## use readlines to read all lines in the file\n",
    "    ## The variable \"lines\" is a list containing all lines\n",
    "    lines = f.readlines()\n",
    "\n",
    "    key_list = []\n",
    "\n",
    "    for line in lines: \n",
    "        data = line.split(\"  \")\n",
    "        key = data[0]\n",
    "        key_list.append(key)\n",
    "\n",
    "    key_list.sort()\n",
    "    length = len(key_list)\n",
    "    print \"num records: \" + str(length)\n",
    "    n = num_workers\n",
    "    key_range = length/n\n",
    "    index = 0\n",
    "    sample_key_list = []\n",
    "    for i in range(1, n+1): #1,2,3\n",
    "        if (i==n):\n",
    "            index = length -1\n",
    "            sample_key_list.append(key_list[length-1])\n",
    "        else:\n",
    "            index += key_range\n",
    "            sample_key_list.append(key_list[index])\n",
    "        print index\n",
    "    \n",
    "    return sample_key_list\n",
    "\n",
    "#sample_keys = get_sample_keys('input_files/input', num_workers)\n",
    "#sample_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# partition stage: partition input data into n groups \n",
    "def mapper(data):\n",
    "    import os\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    sample_keys = data[3]\n",
    "    \n",
    "    #[s3] read from input file: input<id> \n",
    "    s3 = boto3.resource('s3')\n",
    "    key = 'input'+str(id)\n",
    "    file_local = '/tmp/input_tmp'\n",
    "    s3.Bucket(bucket_name).download_file(key, file_local)\n",
    "    \n",
    "    #partition \n",
    "    with open(file_local, \"r\") as f: \n",
    "        lines = f.readlines() #each line contains a 100B record\n",
    "    os.remove(file_local)\n",
    "    #p_list = []\n",
    "    #p_list.append(lines[0:10])\n",
    "    p_list = [[] for x in xrange(n)]  #list of n partitions\n",
    "    for line in lines:\n",
    "        data = line.split(\"  \")\n",
    "        index = 0\n",
    "        while data[0] > sample_keys[index]:\n",
    "            #print index + \" \" + data[0]\n",
    "            index += 1\n",
    "        p_list[index].append(line)\n",
    "\n",
    "        \n",
    "    #write to output files: shuffle<id 0> shuffle<id 1> shuffle<id num_workers-1>\n",
    "    f_list = [] #output file list\n",
    "    \n",
    "    t1 = time.time()\n",
    "    s3_client = boto3.client('s3')\n",
    "    for i in range(n):\n",
    "        file_name = 'shuffle' + str(id) + str(i)\n",
    "        result = s3_client.put_object(\n",
    "            Bucket = bucket_name,\n",
    "            Body = pickle.dumps(p_list[i]),\n",
    "            Key = file_name\n",
    "        )\n",
    "    t2 = time.time()\n",
    "    #return time spent (in sec) writing intermediate files \n",
    "    return t2-t1 \n",
    "\n",
    "#mapper([0, num_workers, bucket_name, sample_keys]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "# sort stage: merge n sets of data & sort \n",
    "def reducer(data):\n",
    "    import os\n",
    "    \n",
    "    id = data[0]\n",
    "    n = num_workers = data[1]\n",
    "    bucket_name = data[2]\n",
    "    \n",
    "    #read from input file: shuffle<0 id> shuffle<1 id> ... shuffle<id num_workers-1>\n",
    "    t1 = time.time()\n",
    "    s3_client = boto3.client('s3')\n",
    "    lines_list = []\n",
    "    for i in range(n):\n",
    "        key = 'shuffle'+ str(0) + str(id)\n",
    "        body = s3_client.get_object(Bucket=bucket_name, Key=key)['Body'].read()\n",
    "        lines = pickle.loads(body)\n",
    "        lines_list.append(lines)\n",
    "    t2 = time.time()\n",
    "    \n",
    "    #merge & sort \n",
    "    merged_lines = sum(lines_list, [])\n",
    "    tuples_list = []\n",
    "    for line in merged_lines:\n",
    "        data = line.split('  ')\n",
    "        tuples_list.append((data[0], data[1]+'  '+data[2]))\n",
    "    \n",
    "    sorted_tuples_list = sorted(tuples_list, key=lambda x: x[0])\n",
    "    \n",
    "    #[s3] write to output file: output<id>  \n",
    "    with open('/tmp/sorted_output', 'w+') as f:\n",
    "        for t in sorted_tuples_list: \n",
    "            f.write(t[0]+'  '+t[1])\n",
    "    \n",
    "    s3_client = boto3.client('s3')\n",
    "    result = s3_client.put_object(\n",
    "        Bucket = bucket_name,\n",
    "        Body = open('/tmp/sorted_output', 'rb'),\n",
    "        Key = 'sorted_output'\n",
    "    )\n",
    "    \n",
    "    #return time (in sec) spent reading intermediate files\n",
    "    return t2-t1\n",
    "\n",
    "#reducer([0, num_workers, bucket_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "To start using `pywren`, we first create an executor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "wrenexec = pywren.default_executor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "You can apply `my_function` to a list of arguments, and each will be executed remotely at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "Future is a placeholder for the returned value from applying `my_function` to the number `3`. We can call `result` on it and get the result. Note that this will block until the remote job has completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num records: 30\n",
      "10\n",
      "20\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "map_data_list = []\n",
    "reduce_data_list = []\n",
    "\n",
    "sample_keys = get_sample_keys('input_files/input', num_workers)\n",
    "\n",
    "for i in range(num_workers):\n",
    "    map_data_list.append([i, num_workers, bucket_name, sample_keys])\n",
    "    reduce_data_list.append([i, num_workers, bucket_name])\n",
    "\n",
    "futures = wrenexec.map(mapper, map_data_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "source": [
    "The pywren `get_all_results` function will wait until all of the futures are done and return their results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08512401580810547, 0.18042778968811035, 0.07289409637451172]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pywren.get_all_results(futures)\n",
    "# returns time spent (in sec) writing intermediate data in each mapper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = wrenexec.map(reducer, reduce_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15877389907836914, 0.1647038459777832, 0.22743606567382812]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pywren.get_all_results(futures)\n",
    "# returns time spent (in sec) reading intermediate data in each reducer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# final stage: concatenate outputs from the reduce/sort stage to form a single sorted output file\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
